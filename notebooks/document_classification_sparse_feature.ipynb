{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "document_classification_sparse_feature.ipynb",
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hu8ItI79905P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tbvI7U7905T",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# Classification of text documents using sparse features\n",
        "\n",
        "\n",
        "This is an example showing how scikit-learn can be used to classify documents\n",
        "by topics using a bag-of-words approach. This example uses a scipy.sparse\n",
        "matrix to store the features and demonstrates various classifiers that can\n",
        "efficiently handle sparse matrices.\n",
        "\n",
        "The dataset used in this example is the 20 newsgroups dataset. It will be\n",
        "automatically downloaded, then cached.\n",
        "\n",
        "The bar plot indicates the accuracy, training time (normalized) and test time\n",
        "(normalized) of each classifier.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nVgpCv9905U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Author: Peter Prettenhofer <peter.prettenhofer@gmail.com>\n",
        "#         Olivier Grisel <olivier.grisel@ensta.org>\n",
        "#         Mathieu Blondel <mathieu@mblondel.org>\n",
        "#         Lars Buitinck\n",
        "# License: BSD 3 clause\n",
        "import logging\n",
        "import numpy as np\n",
        "from optparse import OptionParser\n",
        "import sys\n",
        "from time import time\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "from sklearn.linear_model import RidgeClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.linear_model import PassiveAggressiveClassifier\n",
        "from sklearn.naive_bayes import BernoulliNB, ComplementNB, MultinomialNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.utils.extmath import density\n",
        "from sklearn import metrics"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjR7yb6X905X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "outputId": "4d5ef783-a20b-4a90-a047-6104454d6879"
      },
      "source": [
        "# Display progress logs on stdout\n",
        "logging.basicConfig(level=logging.INFO,\n",
        "                    format='%(asctime)s %(levelname)s %(message)s')\n",
        "\n",
        "\n",
        "# parse commandline arguments\n",
        "op = OptionParser()\n",
        "op.add_option(\"--report\",\n",
        "              action=\"store_true\", dest=\"print_report\",\n",
        "              help=\"Print a detailed classification report.\")\n",
        "op.add_option(\"--chi2_select\",\n",
        "              action=\"store\", type=\"int\", dest=\"select_chi2\",\n",
        "              help=\"Select some number of features using a chi-squared test\")\n",
        "op.add_option(\"--confusion_matrix\",\n",
        "              action=\"store_true\", dest=\"print_cm\",\n",
        "              help=\"Print the confusion matrix.\")\n",
        "op.add_option(\"--top10\",\n",
        "              action=\"store_true\", dest=\"print_top10\",\n",
        "              help=\"Print ten most discriminative terms per class\"\n",
        "                   \" for every classifier.\")\n",
        "op.add_option(\"--all_categories\",\n",
        "              action=\"store_true\", dest=\"all_categories\",\n",
        "              help=\"Whether to use all categories or not.\")\n",
        "op.add_option(\"--use_hashing\",\n",
        "              action=\"store_true\",\n",
        "              help=\"Use a hashing vectorizer.\")\n",
        "op.add_option(\"--n_features\",\n",
        "              action=\"store\", type=int, default=2 ** 16,\n",
        "              help=\"n_features when using the hashing vectorizer.\")\n",
        "op.add_option(\"--filtered\",\n",
        "              action=\"store_true\",\n",
        "              help=\"Remove newsgroup information that is easily overfit: \"\n",
        "                   \"headers, signatures, and quoting.\")\n",
        "\n",
        "\n",
        "def is_interactive():\n",
        "    return not hasattr(sys.modules['__main__'], '__file__')\n",
        "\n",
        "\n",
        "# work-around for Jupyter notebook and IPython console\n",
        "argv = [] if is_interactive() else sys.argv[1:]\n",
        "(opts, args) = op.parse_args(argv)\n",
        "if len(args) > 0:\n",
        "    op.error(\"this script takes no arguments.\")\n",
        "    sys.exit(1)\n",
        "\n",
        "print(__doc__)\n",
        "op.print_help()\n",
        "print()\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Automatically created module for IPython interactive environment\n",
            "Usage: ipykernel_launcher.py [options]\n",
            "\n",
            "Options:\n",
            "  -h, --help            show this help message and exit\n",
            "  --report              Print a detailed classification report.\n",
            "  --chi2_select=SELECT_CHI2\n",
            "                        Select some number of features using a chi-squared\n",
            "                        test\n",
            "  --confusion_matrix    Print the confusion matrix.\n",
            "  --top10               Print ten most discriminative terms per class for\n",
            "                        every classifier.\n",
            "  --all_categories      Whether to use all categories or not.\n",
            "  --use_hashing         Use a hashing vectorizer.\n",
            "  --n_features=N_FEATURES\n",
            "                        n_features when using the hashing vectorizer.\n",
            "  --filtered            Remove newsgroup information that is easily overfit:\n",
            "                        headers, signatures, and quoting.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_tiHf4b905Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "outputId": "0e8651a5-43df-4484-ac59-2e546fbe5109"
      },
      "source": [
        "# #############################################################################\n",
        "# Load some categories from the training set\n",
        "if opts.all_categories:\n",
        "    categories = None\n",
        "else:\n",
        "    categories = [\n",
        "        'alt.atheism',\n",
        "        'talk.religion.misc',\n",
        "        'comp.graphics',\n",
        "        'sci.space',\n",
        "    ]\n",
        "\n",
        "if opts.filtered:\n",
        "    remove = ('headers', 'footers', 'quotes')\n",
        "else:\n",
        "    remove = ()\n",
        "\n",
        "print(\"Loading 20 newsgroups dataset for categories:\")\n",
        "print(categories if categories else \"all\")\n",
        "\n",
        "data_train = fetch_20newsgroups(subset='train', categories=categories,\n",
        "                                shuffle=True, random_state=42,\n",
        "                                remove=remove)\n",
        "\n",
        "data_test = fetch_20newsgroups(subset='test', categories=categories,\n",
        "                               shuffle=True, random_state=42,\n",
        "                               remove=remove)\n",
        "print('data loaded')\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading 20news dataset. This may take a few minutes.\n",
            "2019-10-16 05:09:53,878 INFO Downloading 20news dataset. This may take a few minutes.\n",
            "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n",
            "2019-10-16 05:09:53,882 INFO Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading 20 newsgroups dataset for categories:\n",
            "['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
            "data loaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2O0IwKR905c",
        "colab_type": "text"
      },
      "source": [
        "### Explore the data a bit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjZFHt-y905d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98
        },
        "outputId": "b1493921-3a7d-4692-8138-882ffb9d10a1"
      },
      "source": [
        "# order of labels in `target_names` can be different from `categories`\n",
        "target_names = data_train.target_names\n",
        "\n",
        "\n",
        "def size_mb(docs):\n",
        "    return sum(len(s.encode('utf-8')) for s in docs) / 1e6\n",
        "\n",
        "\n",
        "data_train_size_mb = size_mb(data_train.data)\n",
        "data_test_size_mb = size_mb(data_test.data)\n",
        "\n",
        "print(\"%d documents - %0.3fMB (training set)\" % (\n",
        "    len(data_train.data), data_train_size_mb))\n",
        "print(\"%d documents - %0.3fMB (test set)\" % (\n",
        "    len(data_test.data), data_test_size_mb))\n",
        "print(\"%d categories\" % len(target_names))\n",
        "print(target_names)\n",
        "print()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2034 documents - 3.980MB (training set)\n",
            "1353 documents - 2.867MB (test set)\n",
            "4 categories\n",
            "['alt.atheism', 'comp.graphics', 'sci.space', 'talk.religion.misc']\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArSUgMpr905f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5627c31d-376e-4726-d836-f682a2097b5a"
      },
      "source": [
        "for i in range(0,5):\n",
        "    print(data_train.data[i])\n",
        "\n",
        "for i in range(0,5):\n",
        "    print(data_train.target[i])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "From: rych@festival.ed.ac.uk (R Hawkes)\n",
            "Subject: 3DS: Where did all the texture rules go?\n",
            "Lines: 21\n",
            "\n",
            "Hi,\n",
            "\n",
            "I've noticed that if you only save a model (with all your mapping planes\n",
            "positioned carefully) to a .3DS file that when you reload it after restarting\n",
            "3DS, they are given a default position and orientation.  But if you save\n",
            "to a .PRJ file their positions/orientation are preserved.  Does anyone\n",
            "know why this information is not stored in the .3DS file?  Nothing is\n",
            "explicitly said in the manual about saving texture rules in the .PRJ file. \n",
            "I'd like to be able to read the texture rule information, does anyone have \n",
            "the format for the .PRJ file?\n",
            "\n",
            "Is the .CEL file format available from somewhere?\n",
            "\n",
            "Rych\n",
            "\n",
            "======================================================================\n",
            "Rycharde Hawkes\t\t\t\temail: rych@festival.ed.ac.uk\n",
            "Virtual Environment Laboratory\n",
            "Dept. of Psychology\t\t\tTel  : +44 31 650 3426\n",
            "Univ. of Edinburgh\t\t\tFax  : +44 31 667 0150\n",
            "======================================================================\n",
            "\n",
            "Subject: Re: Biblical Backing of Koresh's 3-02 Tape (Cites enclosed)\n",
            "From: kmcvay@oneb.almanac.bc.ca (Ken Mcvay)\n",
            "Organization: The Old Frog's Almanac\n",
            "Lines: 20\n",
            "\n",
            "In article <20APR199301460499@utarlg.uta.edu> b645zaw@utarlg.uta.edu (stephen) writes:\n",
            "\n",
            ">Seems to me Koresh is yet another messenger that got killed\n",
            ">for the message he carried. (Which says nothing about the \n",
            "\n",
            "Seems to be, barring evidence to the contrary, that Koresh was simply\n",
            "another deranged fanatic who thought it neccessary to take a whole bunch of\n",
            "folks with him, children and all, to satisfy his delusional mania. Jim\n",
            "Jones, circa 1993.\n",
            "\n",
            ">In the mean time, we sure learned a lot about evil and corruption.\n",
            ">Are you surprised things have gotten that rotten?\n",
            "\n",
            "Nope - fruitcakes like Koresh have been demonstrating such evil corruption\n",
            "for centuries.\n",
            "-- \n",
            "The Old Frog's Almanac - A Salute to That Old Frog Hisse'f, Ryugen Fisher \n",
            "     (604) 245-3205 (v32) (604) 245-4366 (2400x4) SCO XENIX 2.3.2 GT \n",
            "  Ladysmith, British Columbia, CANADA. Serving Central Vancouver Island  \n",
            "with public access UseNet and Internet Mail - home to the Holocaust Almanac\n",
            "\n",
            "From: Mark.Perew@p201.f208.n103.z1.fidonet.org\n",
            "Subject: Re: Comet in Temporary Orbit Around Jupiter?\n",
            "X-Sender: newtout 0.08 Feb 23 1993\n",
            "Lines: 15\n",
            "\n",
            "In a message of <Apr 19 04:55>, jgarland@kean.ucs.mun.ca writes:\n",
            "\n",
            " >In article <1993Apr19.020359.26996@sq.sq.com>, msb@sq.sq.com (Mark Brader) \n",
            " >writes:\n",
            "\n",
            "MB>                                                             So the\n",
            "MB> 1970 figure seems unlikely to actually be anything but a perijove.\n",
            "\n",
            "JG>Sorry, _perijoves_...I'm not used to talking this language.\n",
            "\n",
            "Couldn't we just say periapsis or apoapsis?\n",
            "\n",
            " \n",
            "\n",
            "--- msged 2.07\n",
            "\n",
            "From: dpw@sei.cmu.edu (David Wood)\n",
            "Subject: Request for Support\n",
            "Organization: Software Engineering Institute\n",
            "Lines: 35\n",
            "\n",
            "\n",
            "\n",
            "I have a request for those who would like to see Charley Wingate\n",
            "respond to the \"Charley Challenges\" (and judging from my e-mail, there\n",
            "appear to be quite a few of you.)  \n",
            "\n",
            "It is clear that Mr. Wingate intends to continue to post tangential or\n",
            "unrelated articles while ingoring the Challenges themselves.  Between\n",
            "the last two re-postings of the Challenges, I noted perhaps a dozen or\n",
            "more posts by Mr. Wingate, none of which answered a single Challenge.  \n",
            "\n",
            "It seems unmistakable to me that Mr. Wingate hopes that the questions\n",
            "will just go away, and he is doing his level best to change the\n",
            "subject.  Given that this seems a rather common net.theist tactic, I\n",
            "would like to suggest that we impress upon him our desire for answers,\n",
            "in the following manner:\n",
            "\n",
            "1. Ignore any future articles by Mr. Wingate that do not address the\n",
            "Challenges, until he answers them or explictly announces that he\n",
            "refuses to do so.\n",
            "\n",
            "--or--\n",
            "\n",
            "2. If you must respond to one of his articles, include within it\n",
            "something similar to the following:\n",
            "\n",
            "    \"Please answer the questions posed to you in the Charley Challenges.\"\n",
            "\n",
            "Really, I'm not looking to humiliate anyone here, I just want some\n",
            "honest answers.  You wouldn't think that honesty would be too much to\n",
            "ask from a devout Christian, would you?  \n",
            "\n",
            "Nevermind, that was a rhetorical question.\n",
            "\n",
            "--Dave Wood\n",
            "\n",
            "From: prb@access.digex.com (Pat)\n",
            "Subject: Conference on Manned Lunar Exploration.  May 7  Crystal City\n",
            "Organization: Express Access Online Communications, Greenbelt MD USA\n",
            "Lines: 9\n",
            "Distribution: na\n",
            "NNTP-Posting-Host: access.digex.net\n",
            "\n",
            "\n",
            "AW&ST  had a brief blurb on a Manned Lunar Exploration confernce\n",
            "May 7th  at Crystal City Virginia, under the auspices of AIAA.\n",
            "\n",
            "Does anyone know more about this?  How much, to attend????\n",
            "\n",
            "Anyone want to go?\n",
            "\n",
            "pat\n",
            "\n",
            "1\n",
            "3\n",
            "2\n",
            "0\n",
            "2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWmm1YTP905j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        },
        "outputId": "8d84534b-a2c5-477d-f1fa-5fa8a1ee9a08"
      },
      "source": [
        "# split a training set and a test set\n",
        "y_train, y_test = data_train.target, data_test.target\n",
        "\n",
        "print(\"Extracting features from the training data using a sparse vectorizer\")\n",
        "t0 = time()\n",
        "if opts.use_hashing:\n",
        "    vectorizer = HashingVectorizer(stop_words='english', alternate_sign=False,\n",
        "                                   n_features=opts.n_features)\n",
        "    X_train = vectorizer.transform(data_train.data)\n",
        "else:\n",
        "    vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5,\n",
        "                                 stop_words='english')\n",
        "    X_train = vectorizer.fit_transform(data_train.data)\n",
        "duration = time() - t0\n",
        "print(\"done in %fs at %0.3fMB/s\" % (duration, data_train_size_mb / duration))\n",
        "print(\"n_samples: %d, n_features: %d\" % X_train.shape)\n",
        "print()\n",
        "\n",
        "print(\"Extracting features from the test data using the same vectorizer\")\n",
        "t0 = time()\n",
        "X_test = vectorizer.transform(data_test.data)\n",
        "duration = time() - t0\n",
        "print(\"done in %fs at %0.3fMB/s\" % (duration, data_test_size_mb / duration))\n",
        "print(\"n_samples: %d, n_features: %d\" % X_test.shape)\n",
        "print()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting features from the training data using a sparse vectorizer\n",
            "done in 0.500571s at 7.950MB/s\n",
            "n_samples: 2034, n_features: 33809\n",
            "\n",
            "Extracting features from the test data using the same vectorizer\n",
            "done in 0.299312s at 9.580MB/s\n",
            "n_samples: 1353, n_features: 33809\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EjmTY5EE905l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# mapping from integer feature name to original token string\n",
        "if opts.use_hashing:\n",
        "    feature_names = None\n",
        "else:\n",
        "    feature_names = vectorizer.get_feature_names()\n",
        "\n",
        "if opts.select_chi2:\n",
        "    print(\"Extracting %d best features by a chi-squared test\" %\n",
        "          opts.select_chi2)\n",
        "    t0 = time()\n",
        "    ch2 = SelectKBest(chi2, k=opts.select_chi2)\n",
        "    X_train = ch2.fit_transform(X_train, y_train)\n",
        "    X_test = ch2.transform(X_test)\n",
        "    if feature_names:\n",
        "        # keep selected feature names\n",
        "        feature_names = [feature_names[i] for i\n",
        "                         in ch2.get_support(indices=True)]\n",
        "    print(\"done in %fs\" % (time() - t0))\n",
        "    print()\n",
        "\n",
        "if feature_names:\n",
        "    feature_names = np.asarray(feature_names)\n",
        "\n",
        "\n",
        "def trim(s):\n",
        "    \"\"\"Trim string to fit on terminal (assuming 80-column display)\"\"\"\n",
        "    return s if len(s) <= 80 else s[:77] + \"...\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JRGU8Jw905o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# #############################################################################\n",
        "# Benchmark classifiers\n",
        "def benchmark(clf):\n",
        "    print('_' * 80)\n",
        "    print(\"Training: \")\n",
        "    print(clf)\n",
        "    t0 = time()\n",
        "    clf.fit(X_train, y_train)\n",
        "    train_time = time() - t0\n",
        "    print(\"train time: %0.3fs\" % train_time)\n",
        "\n",
        "    t0 = time()\n",
        "    pred = clf.predict(X_test)\n",
        "    test_time = time() - t0\n",
        "    print(\"test time:  %0.3fs\" % test_time)\n",
        "\n",
        "    score = metrics.accuracy_score(y_test, pred)\n",
        "    print(\"accuracy:   %0.3f\" % score)\n",
        "\n",
        "    if hasattr(clf, 'coef_'):\n",
        "        print(\"dimensionality: %d\" % clf.coef_.shape[1])\n",
        "        print(\"density: %f\" % density(clf.coef_))\n",
        "\n",
        "        if opts.print_top10 and feature_names is not None:\n",
        "            print(\"top 10 keywords per class:\")\n",
        "            for i, label in enumerate(target_names):\n",
        "                top10 = np.argsort(clf.coef_[i])[-10:]\n",
        "                print(trim(\"%s: %s\" % (label, \" \".join(feature_names[top10]))))\n",
        "        print()\n",
        "\n",
        "    if opts.print_report:\n",
        "        print(\"classification report:\")\n",
        "        print(metrics.classification_report(y_test, pred,\n",
        "                                            target_names=target_names))\n",
        "\n",
        "    if opts.print_cm:\n",
        "        print(\"confusion matrix:\")\n",
        "        print(metrics.confusion_matrix(y_test, pred))\n",
        "\n",
        "    print()\n",
        "    clf_descr = str(clf).split('(')[0]\n",
        "    return clf_descr, score, train_time, test_time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YvKzqvW905q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9e10d0a4-251c-4f18-970e-46d7cfae9d14"
      },
      "source": [
        "results = []\n",
        "for clf, name in (\n",
        "        (RidgeClassifier(tol=1e-2, solver=\"sag\"), \"Ridge Classifier\"),\n",
        "        (Perceptron(max_iter=50, tol=1e-3), \"Perceptron\"),\n",
        "        (PassiveAggressiveClassifier(max_iter=50, tol=1e-3),\n",
        "         \"Passive-Aggressive\"),\n",
        "        (KNeighborsClassifier(n_neighbors=10), \"kNN\"),\n",
        "        (RandomForestClassifier(n_estimators=100), \"Random forest\")):\n",
        "    print('=' * 80)\n",
        "    print(name)\n",
        "    results.append(benchmark(clf))\n",
        "\n",
        "for penalty in [\"l2\", \"l1\"]:\n",
        "    print('=' * 80)\n",
        "    print(\"%s penalty\" % penalty.upper())\n",
        "    # Train Liblinear model\n",
        "    results.append(benchmark(LinearSVC(penalty=penalty, dual=False,\n",
        "                                       tol=1e-3)))\n",
        "\n",
        "    # Train SGD model\n",
        "    results.append(benchmark(SGDClassifier(alpha=.0001, max_iter=50,\n",
        "                                           penalty=penalty)))\n",
        "\n",
        "# Train SGD with Elastic Net penalty\n",
        "print('=' * 80)\n",
        "print(\"Elastic-Net penalty\")\n",
        "results.append(benchmark(SGDClassifier(alpha=.0001, max_iter=50,\n",
        "                                       penalty=\"elasticnet\")))\n",
        "\n",
        "# Train sparse Naive Bayes classifiers\n",
        "print('=' * 80)\n",
        "print(\"Naive Bayes\")\n",
        "results.append(benchmark(MultinomialNB(alpha=.01)))\n",
        "results.append(benchmark(BernoulliNB(alpha=.01)))\n",
        "results.append(benchmark(ComplementNB(alpha=.1)))\n",
        "\n",
        "print('=' * 80)\n",
        "print(\"LinearSVC with L1-based feature selection\")\n",
        "# The smaller C, the stronger the regularization.\n",
        "# The more regularization, the more sparsity.\n",
        "results.append(benchmark(Pipeline([\n",
        "  ('feature_selection', SelectFromModel(LinearSVC(penalty=\"l1\", dual=False,\n",
        "                                                  tol=1e-3))),\n",
        "  ('classification', LinearSVC(penalty=\"l2\"))])))\n",
        "\n",
        "# make some plots\n",
        "\n",
        "indices = np.arange(len(results))\n",
        "\n",
        "results = [[x[i] for x in results] for i in range(4)]\n",
        "\n",
        "clf_names, score, training_time, test_time = results\n",
        "training_time = np.array(training_time) / np.max(training_time)\n",
        "test_time = np.array(test_time) / np.max(test_time)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.title(\"Score\")\n",
        "plt.barh(indices, score, .2, label=\"score\", color='navy')\n",
        "plt.barh(indices + .3, training_time, .2, label=\"training time\",\n",
        "         color='c')\n",
        "plt.barh(indices + .6, test_time, .2, label=\"test time\", color='darkorange')\n",
        "plt.yticks(())\n",
        "plt.legend(loc='best')\n",
        "plt.subplots_adjust(left=.25)\n",
        "plt.subplots_adjust(top=.95)\n",
        "plt.subplots_adjust(bottom=.05)\n",
        "\n",
        "for i, c in zip(indices, clf_names):\n",
        "    plt.text(-.3, i, c)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "Ridge Classifier\n",
            "________________________________________________________________________________\n",
            "Training: \n",
            "RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,\n",
            "                max_iter=None, normalize=False, random_state=None, solver='sag',\n",
            "                tol=0.01)\n",
            "train time: 0.170s\n",
            "test time:  0.002s\n",
            "accuracy:   0.897\n",
            "dimensionality: 33809\n",
            "density: 1.000000\n",
            "\n",
            "\n",
            "================================================================================\n",
            "Perceptron\n",
            "________________________________________________________________________________\n",
            "Training: \n",
            "Perceptron(alpha=0.0001, class_weight=None, early_stopping=False, eta0=1.0,\n",
            "           fit_intercept=True, max_iter=50, n_iter_no_change=5, n_jobs=None,\n",
            "           penalty=None, random_state=0, shuffle=True, tol=0.001,\n",
            "           validation_fraction=0.1, verbose=0, warm_start=False)\n",
            "train time: 0.030s\n",
            "test time:  0.003s\n",
            "accuracy:   0.888\n",
            "dimensionality: 33809\n",
            "density: 0.255302\n",
            "\n",
            "\n",
            "================================================================================\n",
            "Passive-Aggressive\n",
            "________________________________________________________________________________\n",
            "Training: \n",
            "PassiveAggressiveClassifier(C=1.0, average=False, class_weight=None,\n",
            "                            early_stopping=False, fit_intercept=True,\n",
            "                            loss='hinge', max_iter=50, n_iter_no_change=5,\n",
            "                            n_jobs=None, random_state=None, shuffle=True,\n",
            "                            tol=0.001, validation_fraction=0.1, verbose=0,\n",
            "                            warm_start=False)\n",
            "train time: 0.043s\n",
            "test time:  0.003s\n",
            "accuracy:   0.905\n",
            "dimensionality: 33809\n",
            "density: 0.703311\n",
            "\n",
            "\n",
            "================================================================================\n",
            "kNN\n",
            "________________________________________________________________________________\n",
            "Training: \n",
            "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
            "                     metric_params=None, n_jobs=None, n_neighbors=10, p=2,\n",
            "                     weights='uniform')\n",
            "train time: 0.002s\n",
            "test time:  0.226s\n",
            "accuracy:   0.858\n",
            "\n",
            "================================================================================\n",
            "Random forest\n",
            "________________________________________________________________________________\n",
            "Training: \n",
            "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
            "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
            "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
            "                       min_samples_leaf=1, min_samples_split=2,\n",
            "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
            "                       n_jobs=None, oob_score=False, random_state=None,\n",
            "                       verbose=0, warm_start=False)\n",
            "train time: 1.713s\n",
            "test time:  0.089s\n",
            "accuracy:   0.843\n",
            "\n",
            "================================================================================\n",
            "L2 penalty\n",
            "________________________________________________________________________________\n",
            "Training: \n",
            "LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
            "          multi_class='ovr', penalty='l2', random_state=None, tol=0.001,\n",
            "          verbose=0)\n",
            "train time: 0.136s\n",
            "test time:  0.002s\n",
            "accuracy:   0.900\n",
            "dimensionality: 33809\n",
            "density: 1.000000\n",
            "\n",
            "\n",
            "________________________________________________________________________________\n",
            "Training: \n",
            "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
            "              early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
            "              l1_ratio=0.15, learning_rate='optimal', loss='hinge', max_iter=50,\n",
            "              n_iter_no_change=5, n_jobs=None, penalty='l2', power_t=0.5,\n",
            "              random_state=None, shuffle=True, tol=0.001,\n",
            "              validation_fraction=0.1, verbose=0, warm_start=False)\n",
            "train time: 0.034s\n",
            "test time:  0.003s\n",
            "accuracy:   0.899\n",
            "dimensionality: 33809\n",
            "density: 0.565656\n",
            "\n",
            "\n",
            "================================================================================\n",
            "L1 penalty\n",
            "________________________________________________________________________________\n",
            "Training: \n",
            "LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
            "          multi_class='ovr', penalty='l1', random_state=None, tol=0.001,\n",
            "          verbose=0)\n",
            "train time: 0.308s\n",
            "test time:  0.002s\n",
            "accuracy:   0.873\n",
            "dimensionality: 33809\n",
            "density: 0.005561\n",
            "\n",
            "\n",
            "________________________________________________________________________________\n",
            "Training: \n",
            "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
            "              early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
            "              l1_ratio=0.15, learning_rate='optimal', loss='hinge', max_iter=50,\n",
            "              n_iter_no_change=5, n_jobs=None, penalty='l1', power_t=0.5,\n",
            "              random_state=None, shuffle=True, tol=0.001,\n",
            "              validation_fraction=0.1, verbose=0, warm_start=False)\n",
            "train time: 0.161s\n",
            "test time:  0.003s\n",
            "accuracy:   0.884\n",
            "dimensionality: 33809\n",
            "density: 0.022398\n",
            "\n",
            "\n",
            "================================================================================\n",
            "Elastic-Net penalty\n",
            "________________________________________________________________________________\n",
            "Training: \n",
            "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
            "              early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
            "              l1_ratio=0.15, learning_rate='optimal', loss='hinge', max_iter=50,\n",
            "              n_iter_no_change=5, n_jobs=None, penalty='elasticnet',\n",
            "              power_t=0.5, random_state=None, shuffle=True, tol=0.001,\n",
            "              validation_fraction=0.1, verbose=0, warm_start=False)\n",
            "train time: 0.183s\n",
            "test time:  0.003s\n",
            "accuracy:   0.899\n",
            "dimensionality: 33809\n",
            "density: 0.187583\n",
            "\n",
            "\n",
            "================================================================================\n",
            "Naive Bayes\n",
            "________________________________________________________________________________\n",
            "Training: \n",
            "MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\n",
            "train time: 0.010s\n",
            "test time:  0.002s\n",
            "accuracy:   0.899\n",
            "dimensionality: 33809\n",
            "density: 1.000000\n",
            "\n",
            "\n",
            "________________________________________________________________________________\n",
            "Training: \n",
            "BernoulliNB(alpha=0.01, binarize=0.0, class_prior=None, fit_prior=True)\n",
            "train time: 0.013s\n",
            "test time:  0.014s\n",
            "accuracy:   0.884\n",
            "dimensionality: 33809\n",
            "density: 1.000000\n",
            "\n",
            "\n",
            "________________________________________________________________________________\n",
            "Training: \n",
            "ComplementNB(alpha=0.1, class_prior=None, fit_prior=True, norm=False)\n",
            "train time: 0.012s\n",
            "test time:  0.002s\n",
            "accuracy:   0.911\n",
            "dimensionality: 33809\n",
            "density: 1.000000\n",
            "\n",
            "\n",
            "================================================================================\n",
            "LinearSVC with L1-based feature selection\n",
            "________________________________________________________________________________\n",
            "Training: \n",
            "Pipeline(memory=None,\n",
            "         steps=[('feature_selection',\n",
            "                 SelectFromModel(estimator=LinearSVC(C=1.0, class_weight=None,\n",
            "                                                     dual=False,\n",
            "                                                     fit_intercept=True,\n",
            "                                                     intercept_scaling=1,\n",
            "                                                     loss='squared_hinge',\n",
            "                                                     max_iter=1000,\n",
            "                                                     multi_class='ovr',\n",
            "                                                     penalty='l1',\n",
            "                                                     random_state=None,\n",
            "                                                     tol=0.001, verbose=0),\n",
            "                                 max_features=None, norm_order=1, prefit=False,\n",
            "                                 threshold=None)),\n",
            "                ('classification',\n",
            "                 LinearSVC(C=1.0, class_weight=None, dual=True,\n",
            "                           fit_intercept=True, intercept_scaling=1,\n",
            "                           loss='squared_hinge', max_iter=1000,\n",
            "                           multi_class='ovr', penalty='l2', random_state=None,\n",
            "                           tol=0.0001, verbose=0))],\n",
            "         verbose=False)\n",
            "train time: 0.259s\n",
            "test time:  0.003s\n",
            "accuracy:   0.880\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAuMAAAI1CAYAAAB8GvSWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmIxLkiRJPfn/8Bwy88IPUZcAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 864x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}